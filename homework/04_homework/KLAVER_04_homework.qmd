---
title: "session04 - homework - solutions"
format: html
editor: visual
date: 2025-07-17
author: Timo Roettger
execute:
  error: false
  warning: false
  message: false
  cache: false
bibliography: ../../resources/bibliography.bib
---

# Preamble: Loading packages and configuration

```{r}
#| label: data_and_libraries
#| echo: false

# just run this code chunk
# function to ignoring the setting of the relative path below when knitting
run_if_not_knitting <- function(expr) {
  if (!isTRUE(getOption("knitr.in.progress"))) {
    eval(expr)
  }
}

# nifty code using the pacman package
# it checks if the packages specified below are installed, if not, they will be installed, if yes, they will be loaded
if (!require("pacman")) install.packages("pacman")
pacman::p_load(rstudioapi, tidyverse, lme4, ggeffects, patchwork)

# set the current working directory to the one where this file is
run_if_not_knitting(current_working_dir <- dirname(rstudioapi::getActiveDocumentContext()$path))
run_if_not_knitting(setwd(current_working_dir))

```

# Introduction to data

We will be working with perception data from one of my own experiments [@roettger2014assessing].

In this study, we investigated incomplete neutralization (IN). IN refers to phenomena in which a phonological neutralization is phonetically incomplete. In this paper, researchers looked at final devoicing in German. In German, the word "RÃ¤der" (*engl.* 'wheels') has a voiced /d/, but when forming the singular "Rad" (*engl.* 'wheel'), the /d/ is in word-final position and is devoiced, i.e. loses its phonological voicing and thus is supposed to be identical to the /t/ sound in "Rat" (*engl.* 'council'). However, when measuring the /d/ in "Rad" and the /t/ in "Rat", they are phonetically not identical. There are small but consistent differences between them. In experiment 4 of the paper, the researchers test whether these small acoustic differences can be perceived by naive listeners.

They showed 16 listeners 24 different pseudoword pairs produced by 16 different speakers and measured both reaction time as well as accuracy of correctly identifying the speaker-intended voicing category.

Let's load in the data and have a look:

```{r}
#| label: load-in

incomplete <- read_csv("../../data/incomplete_perception.csv") |> 
  # preprocessing the data such that everything that ought to be a factor is one
  mutate(listener = as.factor(listener),
         voice = as.factor(voice), 
         item_pair = as.factor(item_pair),
         correct_voicing = as.factor(correct_voicing),
         chosen_voicing = as.factor(chosen_voicing)
         )

# have a look
incomplete

```

These are the variables that you will find within the data:

-   `order`: trial order for each listener (1-392)
-   `listener`: unique identifier for each listener (n = 16)
-   `age`: listeners' age
-   `item_pair`: unique identifier for each word pair (e.g. pruk vs. prug, n = 24)
-   `voice`: unique identifier for each speaker voice (n = 16)
-   `RT` = reaction time in ms.
-   `ACC` = accuracy (binary, 1 = correct, 0 = incorrect)
-   `correct_voicing` = whether the audio stimuli contained an underlyingly voiced or voiceless stop in word-final position
-   `chosen_voicing` = the voicing category that the listeners selected in a 2-alternative forced-choice.

### (A) Run a mixed effects model

In their paper, they described different ways how to analyse the perception data. Among other things, they use a generalized linear mixed effects model. While not clearly articulated in the paper, they were mainly interested in whether listeners chose the intended voicing category more often than the unintended one. One way to test this hypothesis is to predict the `chosen_voicing` category by the `correct_voicing` category (i.e. the one that was intended by the speaker).

Build that model with the appropriate random effect structure, taking into account all important grouping variables.

(NOTE: if you encounter the following error message, ignore for now and rerun the model until it does not give this error: *Error in pwrssUpdate(pp, resp, tol = tolPwrss, GQmat = GQmat, compDev = compDev, : Downdated VtV is not positive definite*)

```{r}
#| label: glmer_incomplete

# construct glmem with maximal random effects structure
voicing_mdl <- glmer(chosen_voicing ~ correct_voicing + 
                       # random intercepts and slopes for listeners
                       (1 + correct_voicing | listener) +
                       # random intercepts and slopes for speakers
                       (1 + correct_voicing | voice) +
                       # random intercepts and slopes for word pairs
                       (1 + correct_voicing | item_pair) +
                       # random intercepts and slopes for trial order
                       (1 + correct_voicing | order),
                     data = incomplete,
                     family = 'binomial')
# singular fit, check distributions across grouping variables to decide which slopes to drop

# listener distribution:
listener_summary <- incomplete |>
  group_by(listener) |>
  summarise(correct = sum(correct_voicing == 'voiceless', na.rm = TRUE) / n(),
            chosen = sum(chosen_voicing == 'voiceless', na.rm = TRUE) / n())
listener_summary
barplot(listener_summary$chosen)
sd(listener_summary$chosen) # = 0.044

# speaker distribution:
speaker_summary <- incomplete |>
  group_by(voice) |>
  summarise(correct = sum(correct_voicing == 'voiceless', na.rm = TRUE) / n(),
            chosen = sum(chosen_voicing == 'voiceless', na.rm = TRUE) / n())
speaker_summary
barplot(speaker_summary$chosen)
sd(speaker_summary$chosen) # = 0.187

# word pair distribution:
pair_summary <- incomplete |>
  group_by(item_pair) |>
  summarise(correct = sum(correct_voicing == 'voiceless', na.rm = TRUE) / n(),
            chosen = sum(chosen_voicing == 'voiceless', na.rm = TRUE) / n())
pair_summary
barplot(pair_summary$chosen)
sd(pair_summary$chosen) # = 0.185

# trial order distribution:
order_summary <- incomplete |>
  group_by(order) |>
  summarise(correct = sum(correct_voicing == 'voiceless', na.rm = TRUE) / n(),
            chosen = sum(chosen_voicing == 'voiceless', na.rm = TRUE) / n())
order_summary
barplot(order_summary$chosen)
sd(order_summary$chosen) # 0.127

# simplified model: drop slopes for listeners
voicing_mdl_simpler <- glmer(chosen_voicing ~ correct_voicing + 
                       # random intercepts for listeners
                       (1 | listener) +
                       # random intercepts and slopes for speakers
                       (1 + correct_voicing | voice) +
                       # random intercepts and slopes for word pairs
                       (1 + correct_voicing | item_pair) +
                       # random intercepts and slopes for trial order
                       (1 + correct_voicing | order),
                     data = incomplete,
                     family = 'binomial')
# singular fit, simplyfy again

# simplified model 2: drop slopes for listeners and order
voicing_mdl_simpler2 <- glmer(chosen_voicing ~ correct_voicing + 
                       # random intercepts for listeners
                       (1 | listener) +
                       # random intercepts and slopes for speakers
                       (1 + correct_voicing | voice) +
                       # random intercepts and slopes for word pairs
                       (1 + correct_voicing | item_pair) +
                       # random intercepts for trial order
                       (1 | order),
                     data = incomplete,
                     family = 'binomial')
# singular fit, simplify again

# simplified model 3: drop slopes for listeners, pairs, and order
voicing_mdl_simpler3 <- glmer(chosen_voicing ~ correct_voicing + 
                       # random intercepts for listeners
                       (1 | listener) +
                       # random intercepts and slopes for speakers
                       (1 + correct_voicing | voice) +
                       # random intercepts for word pairs
                       (1 | item_pair) +
                       # random intercepts for trial order
                       (1 | order),
                     data = incomplete,
                     family = 'binomial')
# singular fit, simplify again

# simplified model 4: drop all random slopes
voicing_mdl_intercepts <- glmer(chosen_voicing ~ correct_voicing + 
                       # random intercepts for listeners
                       (1 | listener) +
                       # random intercepts for speakers
                       (1 | voice) +
                       # random intercepts for word pairs
                       (1 | item_pair) +
                       # random intercepts for trial order
                       (1 | order),
                     data = incomplete,
                     family = 'binomial')
```

### (B) Interpret the model output

Interpret the model coefficients and calculate p-values for the relevant predictor to test the hypothesis by comparing the full model (with the predictor) with the corresponding null model (without the predictor, i.e. just substitute the fixed effect predictor with 1).

```{r}
#| label: interpretation

# model summary
summary(voicing_mdl_intercepts)

# interpretation:
## intercept = -0.033 logits; probability of choosing 'voiceless' when true voicing is 'voiced' is 49.2%
## slope = 0.522 logit; probability of choosing 'voiceless' when true voicing is 'voiceless' is 0.522 log odds higher than when true voicing is 'voiced' (=62.0%).

# construct null model for hypothesis testing
null_model <- glmer(chosen_voicing ~ 1 + 
                       # random intercepts for listeners
                       (1 | listener) +
                       # random intercepts for speakers
                       (1 | voice) +
                       # random intercepts for word pairs
                       (1 | item_pair) +
                       # random intercepts for trial order
                       (1 | order),
                     data = incomplete,
                     family = 'binomial')

# anova to compare full and null model
anova(null_model, voicing_mdl_intercepts)
# p-value is <0.0001, probability of encountering the given data if the intended voicing category has no effect on the participants' choice is vanishingly small.
```

### (C) Plot predictions

Plot the model predictions alongside a measure of uncertainty (standard error or 95% CI).

Estimating variance components for linear mixed effects models is mathematically not as straightforward as it is for simple linear models. We shortcut manual extraction for now and use the beautiful `ggeffects` package. Check their `predict_response()` function and use it to derive predictions and plot them.

```{r}
#| label: plot_prediction_in

new_data <- expand.grid(correct_voicing = c("voiced", "voiceless"))
preds <- predict_response(voicing_mdl_intercepts, new_data)

plot <- 
  ggplot(preds, 
         aes(x = x, y = predicted, colour = x)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
                width = 0.2,
                colour = "black") +
  geom_point(size = 5) +
  scale_colour_manual(values = c("#3AB2C1", "#C50D2C")) +
  scale_y_continuous(limits = c(0.25,0.75)) +
  labs(title = "Probability of choosing 'voiceless' based on correct voicing",
       x = "\n Correct voicing",
       y = "Probability of choosing 'voiceless'\n",
       colour = "Correct voicing") +
  theme_minimal()

plot
```

### (D) OPTIONAL Plot variation across grouping variables

Compare the descriptive averages for the critical relationship for your grouping variables and describe what's going on.

```{r}
#| label: plot_descriptive_in

# see code above for plots and SDs of grouping variable variability.

```

We can see massive variation, particularly for different item pairs and different speaker voices.

### (E) Use Github to upload your analysis

-   fork https://github.com/timo-b-roettger/ling4136-advanced-stats
-   clone your forked repo onto your computer
-   store your solutions in this script as `YOURLASTNAME_04_homework.qmd`
-   move your script into `/homework/04_homework/`
-   commit and push changes to your forked repo
-   add me as a collaborator to your repository and send the link to your homework through Canvas

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::
